---
title: "Least Common Denominator Analysis"
author: "Jack Elstner"
date: "2/8/2023"
output: pdf_document
---


**Introduction:** The purpose of this analyses is to examine the effectiveness of the Ocean Ruler tool, an electronic monitoring and reporting platform developed by The Nature Conservancy to aid in fisheries science and management. Poseidon is designed to harness the power of citizen science to generate fisheries data across wider geographies, particularly in data-limited regions. The software allows users to take pictures of fish and invertebrates, then transforms these images into length estimates which can be used to inform management. The platform has been piloted in four different fisheries along the western coast of California and Baja, but little work has been done to characterize the accuracy and precision of the measurements generated by the Poseidon algorithm.

The Poseidon software has been piloted in four fisheries along the coast of California and Baja, including the recreational red abalone fishery of northern California, the recreational groundfish fishery of southern California, the commercial spiny lobster fishery of southern California, and the artisinal pen shell scallop fishery of El Manglito, Baja. In this analysis, I evaluate the accuracy and precision of the Poseidon software across these four fisheries, elucidating factors that influence the quality of length estimates. Through this work, I hope to better understand factors that both improve and limit software performance. 

**Core Research Questions:**

1. How does length estimate accuracy and precision vary across different fisheries?

2. To what degree does making manual corrections improve length estimates?

3. Does the size of the specimen being measured influence length estimate quality?

4. What factors contribute to poor software performance (e.g. size of specimen, species, software user, camera type, image characteristics)

5. Does the user of the Poseidon tool influence length estimate quality? In other words, do some Poseidon users generate better length estimates than others?


**Task 1 - Total Error Across All Fisheries**

The next several code chunks generates box-and-whisker plots of Total Error values across all fisheries for both algorithm and adjusted length estimates. The first plot includes outliers, while the second plot has outlers removed. 

Total Error Boxplot (With Outliers)
```{r}
# load packages
library(ggplot2)
library(ggpubr)

# import Error Spreadsheet
ErrorData <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/CombinedFisheries_Error.csv")

ErrorData$Estimate_Type <- factor(ErrorData$Estimate_Type, levels = c("Algorithm", "Adjusted"))
cols <-c("gray", "red")

# Total Error Boxplot (With Outliers)
TotalError.out = ggplot(dat=ErrorData,
                           aes(x=Fishery, 
                               y=Total_Error_mm, 
                               fill=Estimate_Type)) + 
  geom_boxplot() +
  coord_cartesian(ylim=c(-300, 700)) +
  xlab("Fishery") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Total Error Across Fisheries") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

TotalError.out + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))

```


Total Error Boxplot (No Outliers)
```{r}
# Total Error Boxplot (No Outliers)
TotalError.no_out = ggplot(dat=ErrorData,
                         aes(x=Fishery, 
                             y=Total_Error_mm, 
                             fill=Estimate_Type)) + 
  geom_boxplot(outlier.shape=NA) +
  coord_cartesian(ylim=c(-100, 120)) +
  xlab("Fishery") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Total Error Across Fisheries") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

TotalError.no_out + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))
```




**Task 2 - Percent Error Across All Fisheries**

The next several code chunks generates box-and-whisker plots of Percent Error values across all fisheries for both algorithm and adjusted length estimates. The first plot includes outliers, while the second plot has outlers removed.

Percent Error Boxplots (With Outliers)
```{r}
# Percent Error Boxplot (With Outliers)
PercentError.out = ggplot(dat=ErrorData,
                        aes(x=Fishery, 
                            y=Percent_Error, 
                            fill=Estimate_Type)) + 
  geom_boxplot() +
  coord_cartesian(ylim=c(-200, 800)) +
  xlab("Fishery") +
  ylab("Percent Deviation from True Size (mm)")+
  ggtitle("Percent Error Across Fisheries") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

PercentError.out + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))
```

Percent Error Boxplots (No Outliers)

```{r}
# Percent Error Boxplot (No Outliers)
PercentError.no_out = ggplot(dat=ErrorData,
                           aes(x=Fishery, 
                               y=Percent_Error, 
                               fill=Estimate_Type)) + 
  geom_boxplot(outlier.shape=NA) +
  coord_cartesian(ylim=c(-50, 100)) +
  xlab("Fishery") +
  ylab("Percent Deviation from True Size (%)")+
  ggtitle("Percent Error Across Fisheries") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

PercentError.no_out + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# perform Mood's Median Test with stratification (basically a nonparametric two-way ANOVA)
MoodsMedian.TE <- oneway.test(Total_Error_mm ~ Estimate_Type * Fishery, data = ErrorData)
MoodsMedian.T
# H0:: The groups defined by the combination of both factors (Fishery and EstimateType) have the same median TotalError.
# H1: At least one group defined by the combination of both factors has a different median TotalError.

MoodsMedian.TE # (F = 207.64, num df = 7.0, denom df = 1748.6, p-value < 2.2e-16)

# Since p-value is less than 0.05, I have evidence to reject the null hypothesis. This indicates that there are statistically significant differences in the median TotalError among at least one group combination of Fishery and EstimateType.

# conduct post-hoc analysis to determine which combinations are different from each other. 
# Dunn's test with Bonferroni correction
library(dunn.test)

# this test is to understand the pairwise comparisons and identify which specific group combinations have statistically significant differences in the "Total_Error_mm" variable

posthoc_result <- dunn.test(ErrorData$Total_Error_mm, g = interaction(ErrorData$Estimate_Type, ErrorData$Fishery), method = "bonferroni")

posthoc_result
```




**Task 3 - Accuracy and Precision Metrics Across Fisheries**

The code chunk below calculates mean +/- SD of total error and percent error for each Poseidon estimate type. 

```{r}
# read in each fishery dataset 
abalone <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/Fishery-Specific Data/csv/AbaloneCleanedJanuary2023.csv")

finfish <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/Fishery-Specific Data/csv/CaliforniaFinfish_March2_Merged&Cleaned.csv")

lobster <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/Fishery-Specific Data/csv/CaliforniaSpinyLobster_Cleaned_Jan2023.csv")

penshell <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/Fishery-Specific Data/csv/Penshell_Cleaned_January2023.csv")


# subset to only include yes's
abalone.yes <- subset(abalone, Include == "yes")
finfish.yes <- subset(finfish, Include == "yes")
lobster.yes <- subset(lobster, Include == "yes")
penshell.yes <- subset(penshell, Included == "yes")

# abalone accuracy/precision metrics
mean(abalone.yes$Algorithm_Total_Error_mm) # mean algorithm TE = 23.306
median(abalone.yes$Algorithm_Total_Error_mm) # median algorithm TE = 14.43
sd(abalone.yes$Algorithm_Total_Error_mm) # sd algorithm TE = 38.254

mean(abalone.yes$Adjusted_Total_Error_mm) # mean adjusted TE = -1.655
median(abalone.yes$Adjusted_Total_Error_mm) # median algorithm TE = -1.29
sd(abalone.yes$Adjusted_Total_Error_mm)# sd adjusted TE = 13.061

mean(abalone.yes$Algorithm_Percent_Error) # mean algorithm PE = 10.705
median(abalone.yes$Algorithm_Percent_Error) # median algorithm PE = 6.435
sd(abalone.yes$Algorithm_Percent_Error) # sd algorithm PE = 17.518

mean(abalone.yes$Adjusted_Percent_Error) # mean adjusted PE = -0.660
median(abalone.yes$Adjusted_Percent_Error) # median adjusted PE = -0.58
sd(abalone.yes$Adjusted_Percent_Error) # sd adjusted PE = 5.906


# finfish accuracy/precision metrics
mean(finfish.yes$Algorithm_Total_Error_mm) # mean algorithm TE = 29.622
median(finfish.yes$Algorithm_Total_Error_mm)
sd(finfish.yes$Algorithm_Total_Error_mm) # sd algorithm TE = 49.254

mean(finfish.yes$Adjusted_Total_Error_mm) # mean adjusted TE = 41.013
sd(finfish.yes$Adjusted_Total_Error_mm) # sd adjusted TE = 22.271

mean(finfish.yes$Algorithm_Percent_Error) # mean algorithm PE = 10.999
sd(finfish.yes$Algorithm_Percent_Error) # sd algorithm PE = 16.552

mean(finfish.yes$Adjusted_Percent_Error) # mean adjusted PE = 14.209
sd(finfish.yes$Adjusted_Percent_Error) # sd adjusted PE = 7.697


# lobster accuracy/precision metrics
mean(lobster.yes$Algorithm_Total_Error_mm) # mean algorithm TE = 17.579
sd(lobster.yes$Algorithm_Total_Error_mm) # sd algorithm TE = 28.227

mean(lobster.yes$Adjusted_Total_Error_mm) # mean adjusted TE = 1.712
sd(lobster.yes$Adjusted_Total_Error_mm) # sd adjusted TE = 8.427

mean(lobster.yes$Algorithm_Percent_Error) # mean algorithm PE = 18.015
sd(lobster.yes$Algorithm_Percent_Error) # sd algorithm PE = 26.873

mean(lobster.yes$Adjusted_Percent_Error) # mean adjusted PE = 1.604
sd(lobster.yes$Adjusted_Percent_Error) # sd adjusted PE = 8.354


# penshell accuracy/precision metrics
mean(penshell.yes$Algorithm_Total_Error_mm) # mean algorithm TE = 12.159
sd(penshell.yes$Algorithm_Total_Error_mm) # sd algorithm TE = 83.893

mean(penshell.yes$Adjusted_Total_Error_mm) # mean adjusted TE = 1.926
sd(penshell.yes$Adjusted_Total_Error_mm) # sd adjusted TE = 14.640

mean(penshell.yes$Algorithm_Percent_Error) # mean algorithm PE = 9.017
sd(penshell.yes$Algorithm_Percent_Error) # sd algorithm PE = 57.661

mean(penshell.yes$Adjusted_Percent_Error) # mean adjusted PE = 1.664
sd(penshell.yes$Adjusted_Percent_Error) # sd adjusted PE = 11.864
```




The next several code chunks asks the same questions as above, but uses a Bayesian framework
```{r}
# load packages
library(ggplot2)
library(ggpubr)

# import Error Spreadsheet
ErrorData <- read.csv("~/Desktop/SIO/Research/Poseidon Project/Final Analysis/Final Data/CombinedFisheries_Error.csv")

# filter for each type of error measurement across all fisheries

# abalone
abalone.algorithm.error <- subset(ErrorData, Estimate_Type == "Algorithm" & Fishery == "Abalone")
abalone.adjusted.error <- subset(ErrorData, Estimate_Type == "Adjusted" & Fishery == "Abalone")

# finfish
finfish.algorithm.error <- subset(ErrorData, Estimate_Type == "Algorithm" & Fishery == "Finfish")
finfish.adjusted.error <- subset(ErrorData, Estimate_Type == "Adjusted" & Fishery == "Finfish")

# lobster
lobster.algorithm.error <- subset(ErrorData, Estimate_Type == "Algorithm" & Fishery == "Lobster")
lobster.adjusted.error <- subset(ErrorData, Estimate_Type == "Adjusted" & Fishery == "Lobster")

# penshell
penshell.algorithm.error <- subset(ErrorData, Estimate_Type == "Algorithm" & Fishery == "Penshell")
penshell.adjusted.error <- subset(ErrorData, Estimate_Type == "Adjusted" & Fishery == "Penshell")
```


abalone model fitting and posterior extraction
```{r}
library(rethinking)

# abalone algorithm model
abaloneAlgorithm <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 30),
    sigma ~ dunif(0, 50)
  ), data = abalone.algorithm.error
)

abaloneAlgorithm.post <- extract.samples(abaloneAlgorithm, n = 1000)


# abalone adjusted model
abaloneAdjusted <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 30),
    sigma ~ dunif(0, 50)
  ), data = abalone.adjusted.error
)

abaloneAdjusted.post <- extract.samples(abaloneAdjusted, n = 1000)
```

finfish model fitting and posterior extraction
```{r}
# finfish algorithm model
finfishAlgorithm <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 50),
    sigma ~ dunif(0, 50)
  ), data = finfish.algorithm.error
)

finfishAlgorithm.post <- extract.samples(finfishAlgorithm, n = 1000)


# finfish adjusted model
finfishAdjusted <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 50),
    sigma ~ dunif(0, 50)
  ), data = finfish.adjusted.error
)

finfishAdjusted.post <- extract.samples(finfishAdjusted, n = 1000)
```

lobster model fitting and posterior extraction
```{r}
# lobster algorithm model
lobsterAlgorithm <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 30),
    sigma ~ dunif(0, 50)
  ), data = lobster.algorithm.error
)

lobsterAlgorithm.post <- extract.samples(lobsterAlgorithm, n = 1000)


# lobster adjusted model
lobsterAdjusted <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 30),
    sigma ~ dunif(0, 50)
  ), data = lobster.adjusted.error
)

lobsterAdjusted.post <- extract.samples(lobsterAdjusted, n = 1000)
```

penshell model fitting and posterior extraction
```{r}
# penshell algorithm model
penshellAlgorithm <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 60),
    sigma ~ dunif(0, 100)
  ), data = penshell.algorithm.error
)

penshellAlgorithm.post <- extract.samples(penshellAlgorithm, n = 1000)


# penshell adjusted model
penshellAdjusted <- quap(
  alist(
    Total_Error_mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 60),
    sigma ~ dunif(0, 100)
  ), data = penshell.adjusted.error
)

penshellAdjusted.post <- extract.samples(penshellAdjusted, n = 1000)
```

posterior data table creation - total error
```{r}
var.names <- c("Total_Error_mm", "Estimate_Type", "Fishery")

# ABALONE
# abalone algorithm dataframe
abaloneAlgorithmPost <- abaloneAlgorithm.post$mu
alg.label <- rep("Algorithm", 1000)
abalone.label <- rep("Abalone", 1000)
abalone.alg.df <- data.frame(abaloneAlgorithmPost, alg.label, abalone.label)
names(abalone.alg.df) <- var.names

# abalone adjusted dataframe
abaloneAdjustedPost <- abaloneAdjusted.post$mu
adj.label <- rep("Adjusted", 1000)
abalone.label <- rep("Abalone", 1000)
abalone.adj.df <- data.frame(abaloneAdjustedPost, adj.label, abalone.label)
names(abalone.adj.df) <- var.names




# FINFISH
# finfish algorithm dataframe
finfishAlgorithmPost <- finfishAlgorithm.post$mu
alg.label <- rep("Algorithm", 1000)
finfish.label <- rep("Finfish", 1000)
finfish.alg.df <- data.frame(finfishAlgorithmPost, alg.label, finfish.label)
names(finfish.alg.df) <- var.names

# abalone adjusted dataframe
finfishAdjustedPost <- finfishAdjusted.post$mu
adj.label <- rep("Adjusted", 1000)
finfish.label <- rep("Finfish", 1000)
finfish.adj.df <- data.frame(finfishAdjustedPost, adj.label, finfish.label)
names(finfish.adj.df) <- var.names




# LOBSTER
# lobster algorithm dataframe
lobsterAlgorithmPost <- lobsterAlgorithm.post$mu
alg.label <- rep("Algorithm", 1000)
lobster.label <- rep("Lobster", 1000)
lobster.alg.df <- data.frame(lobsterAlgorithmPost, alg.label, lobster.label)
names(lobster.alg.df) <- var.names

# lobster adjusted dataframe
lobsterAdjustedPost <- lobsterAdjusted.post$mu
adj.label <- rep("Adjusted", 1000)
lobster.label <- rep("Lobster", 1000)
lobster.adj.df <- data.frame(lobsterAdjustedPost, adj.label, lobster.label)
names(lobster.adj.df) <- var.names




# PENSHELL
# penshell algorithm dataframe
penshellAlgorithmPost <- penshellAlgorithm.post$mu
alg.label <- rep("Algorithm", 1000)
penshell.label <- rep("Penshell", 1000)
penshell.alg.df <- data.frame(penshellAlgorithmPost, alg.label, penshell.label)
names(penshell.alg.df) <- var.names

# penshell adjusted dataframe
penshellAdjustedPost <- penshellAdjusted.post$mu
adj.label <- rep("Adjusted", 1000)
penshell.label <- rep("Penshell", 1000)
penshell.adj.df <- data.frame(penshellAdjustedPost, adj.label, penshell.label)
names(penshell.adj.df) <- var.names

posteriorTE.df <- rbind(abalone.alg.df,
                        abalone.adj.df,
                        finfish.alg.df,
                        finfish.adj.df,
                        lobster.alg.df,
                        lobster.adj.df,
                        penshell.alg.df,
                        penshell.adj.df)
```

make total error barplot using Bayesian posteriors
```{r}
cols <-c("red", "gray")

TotalErrorBayesian = ggplot(dat=posteriorTE.df,
                           aes(x=Fishery, 
                               y=Total_Error_mm, 
                               fill=Estimate_Type)) + 
  geom_boxplot() +
  coord_cartesian(ylim=c(-5, 45)) +
  xlab("Fishery") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Total Error Across Fisheries - Bayesian Posteriors") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

TotalErrorBayesian + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))
```

**Key Findings:** Through the preliminary analyses above, I have demonstrated that:

1) Total Error values are influenced by both Fishery and Estimate Type
2) Percent Error values are influenced by both Fishery and Estimate Type
3) Manual corrections almost always reduce Total Error values (except for finfish) and reduce Total Error variability 
4) Manual corrections almost always reduce Percent Error values (except for finfish) and reduce Percent Error variability
5) Both algorithm and adjusted length estimates tend to overestimate the true sizes of specimens





**Impact of Specimen Size on Length Estimate Quality:** The next several code chunks explore relationships between algorithm accuracy and the size of the specimen being measured. To do this, I fit simple linear regressions between Algorithm/Adjusted Total Error and the true size of a specimen. In this context, Total Error represents the difference between the algorithm-generated length estimate and the true size of the animal. Regression models are fit via quadratic approximation, using the quap function in the rethinking package. 

If the software is performing well, then we should expect to see no relationship between algorithm deviance and true size, i.e. the slope of the regression line should be zero across all fisheries. 


**Abalone Model Fitting**

abalone prior predictive check - Algorithm TE vs. True Size
```{r}
library(rethinking)

# read in abalone dataset and subset to only include yes's
abalone.all <- read_excel("Final Data/Fishery-Specific Data/AbaloneCleanedJanuary2023.xls")
Abalone <- subset(abalone.all, Include == "yes")

# standardize variables of interest
abalone$AlgorithmTotalError.z <- scale(abalone$Algorithm_Total_Error_mm)
abalone$HandMeasurement.z <- scale(abalone$Hand_Measurement_mm)

N <- 100
a <- rnorm(N, 0, 1)
b <- rnorm(N, 0, 0.5)

plot(NULL, xlim = range(abalone$HandMeasurement.z), ylim = range(abalone$AlgorithmTotalError.z), 
xlab = "True Size (Standardized)", ylab = "Algorithm Total Error (Standardized)")
abline(h=0, lty=2)
abline(h=272, lty=1, lwd=0.5)
xbar <- mean(abalone$HandMeasurement.z)
for (i in 1:N) curve(a[i] + b[i]*x,
                     from=min(abalone$HandMeasurement.z), to = max(abalone$HandMeasurement.z), add = TRUE,                                                        col=col.alpha("black", 0.2))
```

fit abalone model and calculate MAP/HPDI
```{r}
# fit abalone model
abalone.lm <- quap(
  alist(
    AlgorithmTotalError.z ~ dnorm(mu, sigma),
    mu <- a + b*(HandMeasurement.z),
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = abalone
)

precis(abalone.lm)

abalone.post <- extract.samples(abalone.lm, n = 10000)
dens(abalone.post$b, main = "Slope Posterior Distibrution (Abalone)")

# calculate maximum a posteriori estimate (MAP)
round(chainmode(abalone.post$b), digits = 3) # -0.146

# calculate Highest Probability Density Interval (HPDI)
round(HPDI(abalone.post$b, prob = 0.95), digits = 3) # 95% HPDI = [-0.224, -0.057]
```


quick posterior predictive check - abalone
```{r}
plot(AlgorithmTotalError.z ~ HandMeasurement.z, data=abalone, col=rangi2, main = "Abalone")
abalone.post <- extract.samples(abalone.lm, n = 10000)
a_map <- mean(abalone.post$b)
b_map <- mean(abalone.post$b)
curve(a_map + b_map*x , add=TRUE)

# the best fit line generated seems reasonable
```



**Finfish Model Fitting**


fit finfish model and calculate MAP/HPDI
```{r}
# read in finfish dataset and subset to only include yes's
finfish.all <- read_excel("Final Data/Fishery-Specific Data/CaliforniaFinfish_March2_Merged&Cleaned.xlsx")
Finfish <- subset(finfish.all, Include == "yes")

# standardize variables of interest
Finfish$AlgorithmTotalError.z <- scale(Finfish$Algorithm_Total_Error_mm)
Finfish$HandMeasurement.z <- scale(Finfish$Scientist_Hand_Measured_TL_mm)

# fit finfish model
finfish.lm <- quap(
  alist(
    AlgorithmTotalError.z ~ dnorm(mu, sigma),
    mu <- a + b*(HandMeasurement.z),
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = Finfish
)

precis(finfish.lm)

finfish.post <- extract.samples(finfish.lm, n = 10000)
dens(finfish.post$b, main = "Slope Posterior Distibrution (Finfish)")

# calculate maximum a posteriori estimate (MAP)
round(chainmode(finfish.post$b), digits = 3) # -0.009

# calculate Highest Probability Density Interval (HPDI)
round(HPDI(finfish.post$b, prob = 0.95), digits = 3) # 95% HPDI = [-0.126, 0.090]
```





quick posterior predictive check - finfish
```{r}
plot(AlgorithmTotalError.z ~ HandMeasurement.z, data=Finfish, col=rangi2, main = "Finfish")
finfish.post <- extract.samples(finfish.lm, n = 10000)
a_map <- mean(finfish.post$b)
b_map <- mean(finfish.post$b)
curve(a_map + b_map*x , add=TRUE)

# the best fit line generated seems reasonable
```




**Lobster Model Fitting**

fit lobster model and calculate MAP/HPDI
```{r}
# read in lobster dataset and subset to only include yes's
lobster.all <- read_excel("Final Data/Fishery-Specific Data/CaliforniaSpinyLobster_Cleaned_Jan2023.xlsx")
Lobster <- subset(lobster.all, Include == "yes")

# standardize variables of interest
Lobster$AlgorithmTotalError.z <- scale(Lobster$Algorithm_Total_Error_mm)
Lobster$HandMeasurement.z <- scale(Lobster$Hand_Measured_Carapace_Length_mm)

# fit lobster model
lobster.lm <- quap(
  alist(
    AlgorithmTotalError.z ~ dnorm(mu, sigma),
    mu <- a + b*(HandMeasurement.z),
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = Lobster
)

precis(lobster.lm)

lobster.post <- extract.samples(lobster.lm, n = 10000)
dens(lobster.post$b, main = "Slope Posterior Distibrution (Lobster)")

# calculate maximum a posteriori estimate (MAP)
round(chainmode(lobster.post$b), digits = 3) # MAP = -0.088

# calculate Highest Probability Density Interval (HPDI)
round(HPDI(lobster.post$b, prob = 0.95), digits = 3) # 95% HPDI = [-0.160, -0.018]
```


quick posterior predictive check - lobster
```{r}
plot(AlgorithmTotalError.z ~ HandMeasurement.z, data=Lobster, col=rangi2, main = "Lobster")
lobster.post <- extract.samples(lobster.lm, n = 10000)
a_map <- mean(lobster.post$b)
b_map <- mean(lobster.post$b)
curve(a_map + b_map*x , add=TRUE)

# the best fit line generated seems reasonable
```



**Penshell Model Fitting**

fit penshell model and calculate MAP/HPDI
```{r}
# read in lobster dataset and subset to only include yes's
penshell.all <- read_excel("Final Data/Fishery-Specific Data/Penshell_Cleaned_January2023.xls")
Penshell <- subset(penshell.all, Included == "yes")

# standardize variables of interest
Penshell$AlgorithmTotalError.z <- scale(Penshell$Algorithm_Total_Error_mm)
Penshell$HandMeasurement.z <- scale(Penshell$Hand_Measurement_mm)

# fit lobster model
penshell.lm <- quap(
  alist(
    AlgorithmTotalError.z ~ dnorm(mu, sigma),
    mu <- a + b*(HandMeasurement.z),
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = Penshell
)

precis(penshell.lm)

penshell.post <- extract.samples(penshell.lm, n = 10000)
dens(penshell.post$b, main = "Slope Posterior Distibrution (Penshell)")

# calculate maximum a posteriori estimate (MAP)
round(chainmode(penshell.post$b), digits = 3) # MAP = 0.148

# calculate Highest Probability Density Interval (HPDI)
round(HPDI(penshell.post$b, prob = 0.95), digits = 3) # 95% HPDI = [0.099, 0.202]
```



quick posterior predictive check - penshell
```{r}
plot(AlgorithmTotalError.z ~ HandMeasurement.z, data=Penshell, col=rangi2, main = "Penshell")
penshell.post <- extract.samples(penshell.lm, n = 10000)
a_map <- mean(penshell.post$b)
b_map <- mean(penshell.post$b)
curve(a_map + b_map*x , add=TRUE)

# this model is very sensitive to outliers. if the outliers are removed, the slope is much closer to zero, or even slightly negative
```



Make a figure displaying posterior density plots for slope parameter estimates across all four fisheries
```{r}
par(mfrow=c(2,2))

dens(abalone.post$b, xlim=c(-1, 1), main = "Abalone")
abline(v=0, col = "blue")

dens(finfish.post$b, xlim=c(-1, 1), main = "Finfish")
abline(v=0, col = "blue")

dens(lobster.post$b, xlim=c(-1, 1), main = "Lobster")
abline(v=0, col = "blue")

dens(penshell.post$b, xlim=c(-1, 1), main = "Penshell")
abline(v=0, col = "blue")
```





**Exploring Variability Between Users**

```{r}
# read in abalone, finfish, and penshell data
AbaloneCleanedJanuary2023 <- read_excel("Final Data/Fishery-Specific Data/AbaloneCleanedJanuary2023.xls")
CaliforniaFinfish_March2_Merged_Cleaned <- read_excel("Final Data/Fishery-Specific Data/CaliforniaFinfish_March2_Merged&Cleaned.xlsx")
Penshell_Cleaned_January2023 <- read_excel("Final Data/Fishery-Specific Data/Penshell_Cleaned_January2023.xls")

# subset to only include yes's
abalone <- subset(AbaloneCleanedJanuary2023, Include == "yes")
finfish <- subset(CaliforniaFinfish_March2_Merged_Cleaned, Include == "yes")
penshell <- subset(Penshell_Cleaned_January2023, Included == "yes")
```


variability between users figure
```{r}

# abalone users plot (algorithm)
abalone.alg.plot = ggplot(dat=abalone,
      aes(x=User_ID, 
      y=Algorithm_Total_Error_mm,
      group = User_ID)) +
  geom_boxplot(outlier.shape=NA, color = "gray") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Abalone") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

# abalone users plot (adjusted)
abalone.adj.plot = ggplot(dat=abalone,
      aes(x=User_ID, 
      y=Adjusted_Total_Error_mm,
      group = User_ID)) +
  geom_boxplot(outlier.shape=NA, color = "red") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Abalone") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()



# finfish users plot (algorithm)
finfish.alg.plot = ggplot(dat=finfish,
        aes(x=AnglerID, 
          y=Algorithm_Total_Error_mm,
          group = AnglerID)) +
  geom_boxplot(outlier.shape=NA, color = "gray") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Finfish") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()


# finfish users plot (adjusted)
finfish.adj.plot = ggplot(dat=finfish,
        aes(x=AnglerID, 
          y=Adjusted_Total_Error_mm,
          group = AnglerID)) +
  geom_boxplot(outlier.shape=NA, color = "red") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Finfish") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()




# penshell users plot (algorithm)
penshell.alg.plot = ggplot(dat=penshell,
      aes(x=User_ID, 
          y=Algorithm_Total_Error_mm,
          group = User_ID)) +
  geom_boxplot(outlier.shape=NA, color = "gray") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Penshell") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()


# penshell users plot (adjusted)
penshell.adj.plot = ggplot(dat=penshell,
      aes(x=User_ID, 
          y=Adjusted_Total_Error_mm,
          group = User_ID)) +
  geom_boxplot(outlier.shape=NA, color = "red") +
  coord_cartesian(ylim=c(-120, 175)) +
  xlab("Software User ID") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Penshell") +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()
```


```{r}
library(ggplot2)
library(ggpubr)
user_figure <- ggarrange(abalone.alg.plot, finfish.alg.plot, penshell.alg.plot,
                         abalone.adj.plot, finfish.adj.plot, penshell.adj.plot,
          nrow=2,
          ncol=3)
user_figure
```


**Exploring Camera Effects**

Below I make a boxplot showing differences in accuracy across the four different smartphone camera types that we have specs for (Samsung Galaxy 6s, iPhone 5, iPhone 5s, iPhone 6). This analysis only includes yes's.

```{r}
# load packages
library(ggplot2)
library(ggpubr)

# import data
abaloneCameraStacked <- read_excel("Final Data/abaloneCameraStacked.xlsx")
abaloneCameraStacked$EstimateType <- factor(abaloneCameraStacked$EstimateType, levels = c("Algorithm", "Adjusted"))
cols <-c("gray", "red")

# Camera Effects Boxplot (No Outliers)
CameraEffectsPlot = ggplot(dat=abaloneCameraStacked,
                         aes(x= CameraType, 
                             y=TotalError_mm, 
                             fill=EstimateType)) + 
  geom_boxplot(outlier.shape=NA) +
  coord_cartesian(ylim=c(-100, 120)) +
  xlab("Camera Type") +
  ylab("Deviation from True Size (mm)")+
  ggtitle("Total Error Across Camera Types") +
  scale_fill_manual(values = cols) +
  geom_hline(yintercept=0,linetype=2) +
  theme_bw()

CameraEffectsPlot + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  guides(fill=guide_legend(title="Algorithm Estimate Type")) +
  theme(plot.title = element_text(hjust = 0.5))
```




